<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">

<head>
		<meta http-equiv="content-type" content="text/html;charset=utf-8" />
		<meta name="generator" content="Adobe GoLive" />
<title>Oded Ghitza</title>
		
		
	<link href="styles.css" rel="stylesheet" type="text/css">
	<script type="text/javascript">
function MM_preloadImages() { //v3.0
  var d=document; if(d.images){ if(!d.MM_p) d.MM_p=new Array();
    var i,j=d.MM_p.length,a=MM_preloadImages.arguments; for(i=0; i<a.length; i++)
    if (a[i].indexOf("#")!=0){ d.MM_p[j]=new Image; d.MM_p[j++].src=a[i];}}
}
function MM_swapImgRestore() { //v3.0
  var i,x,a=document.MM_sr; for(i=0;a&&i<a.length&&(x=a[i])&&x.oSrc;i++) x.src=x.oSrc;
}
function MM_findObj(n, d) { //v4.01
  var p,i,x;  if(!d) d=document; if((p=n.indexOf("?"))>0&&parent.frames.length) {
    d=parent.frames[n.substring(p+1)].document; n=n.substring(0,p);}
  if(!(x=d[n])&&d.all) x=d.all[n]; for (i=0;!x&&i<d.forms.length;i++) x=d.forms[i][n];
  for(i=0;!x&&d.layers&&i<d.layers.length;i++) x=MM_findObj(n,d.layers[i].document);
  if(!x && d.getElementById) x=d.getElementById(n); return x;
}

function MM_swapImage() { //v3.0
  var i,j=0,x,a=MM_swapImage.arguments; document.MM_sr=new Array; for(i=0;i<(a.length-2);i+=3)
   if ((x=MM_findObj(a[i]))!=null){document.MM_sr[j++]=x; if(!x.oSrc) x.oSrc=x.src; x.src=a[i+2];}
}
    </script>
	</head>

<body onLoad="MM_preloadImages('includes/images/home_hover.gif','includes/images/research_hover.gif','includes/images/publication_hover.gif','includes/images/contact_hover.gif','images/research_on.gif','images/publications_on.gif','images/contactus_on.gif','images/home_on.gif')">
		<table width="796" height="100" border="0" cellpadding="0" cellspacing="0" align="center">
<tr>
		<td colspan="8" width="594" height="76">
			</td>
		<td rowspan="2"><a href="index.html" target="_self"><img src="images/oded_ghitza_logo.gif" alt="" width="202" height="100" border="0"></a></td>
	</tr>
	<tr>
		<td><a href="index.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage('Image13','','images/home_on.gif',1)"><img src="images/home_off.gif" name="Image13" width="93" height="24" border="0" id="Image13" /></a></td>
		<td>
			<img src="images/dots.gif" width="8" height="24" alt=""></td>
		<td><a href="research.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage('Image10','','images/research_on.gif',1)"><img src="images/research_off.gif" name="Image10" width="97" height="24" border="0" id="Image10" /></a></td>
		<td><img src="images/dots.gif" width="8" height="24" alt=""></td>
		<td><a href="publications.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage('Image11','','images/publications_on.gif',1)"><img src="images/publications_off.gif" name="Image11" width="125" height="24" border="0" id="Image11" /></a></td>
		<td><img src="images/dots.gif" width="8" height="24" alt=""></td>
		<td><a href="contact.html" onmouseout="MM_swapImgRestore()" onmouseover="MM_swapImage('Image12','','images/contactus_on.gif',1)"><img src="images/contactus_off.gif" name="Image12" width="104" height="24" border="0" id="Image12" /></a></td>
		<td>
			<img src="images/spacer.gif" width="155" height="24" alt=""></td>
	</tr>
</table>
<table align="center"><tr>
  <td><img src="images/publications_hero.gif" width="796" height="166"></td></tr></table>


<table width="700" border="0" align="center" cellpadding="6" cellspacing="0" class="bodylg">

  <tr>
    <td class="subhead"><strong>Peer-Reviewed Articles</strong></td>
  </tr>
  <tr>
    <td class="bodylg" valign="top"> 
   <p><strong> Ghitza, O.</strong> (2016). &quot;Acoustic-driven delta rhythms as prosodic markers&quot; <em>Language, Cognition and Neuroscience</em>. http://dx.doi.org/10.1080/23273798.2016.1232419<br /></p>
   <p>Farbood, M. F., Rowland, J., Marcus, G., <strong> Ghitza, O.</strong> and Poeppel, D. (2014). &quot;Decoding time for the identification of musical key.&quot; <em>Atten Percept Psychophys</em>. doi: 10.3758/s13414-014-0806-0<br />
[ <a href="downloads/peer-reviewed articles/Farbood_etal_14.pdf" target="_new" class="linklg">PDF File</a> ]</p>
     <p><strong> Ghitza, O.</strong> (2014). &quot;Behavioral evidence for the role of cortical theta oscillations in determining auditory channel capacity for speech.&quot; <em>Front. Psychol.</em>  5:652. doi:10.3389/fpsyg.2014.00652<br />
[ <a href="downloads/peer-reviewed articles/Ghitza_14.pdf" target="_new" class="linklg">PDF File</a> ]</p>    
    <p>Jepsen, M. L., Dau, T. and <strong> Ghitza, O.</strong> (2014). &quot;Refining a model of hearing impairment using speech psychophysics.&quot; <em> J. Acoust. Soc. Am.</em>, 135 (4), EL179, http://dx.doi.org/10.1121/1.4869256<br />
     [ <a href="downloads/peer-reviewed articles/JepsenDauGhitza_14.pdf" target="_new" class="linklg">PDF File</a> ]</br>
    <p> Doelling, K. B., Arnal, L. H., <strong> Ghitza, O.</strong> and Poeppel, D. (2014). &quot;Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual parsing.&quot; <em>NeuroImage.</em> 85:761. doi: 10.1016/j.neuroimage.2013.06.035<br />
 [ <a href="downloads/peer-reviewed articles/Doelling_etal_14.pdf" target="_new" class="linklg">PDF File</a> ]</br>   
   <p><strong> Ghitza, O.</strong> (2013). &quot;The theta-syllable: a unit of speech information defined by cortical function.&quot; <em>Front. Psychol.</em> 4:138. doi: 10.3389/fpsyg.2013.00138<br />
 [ <a href="downloads/peer-reviewed articles/Ghitza_13.pdf" target="_new" class="linklg">PDF File</a> ]</br>
    <p><strong> Ghitza, O.</strong>, Giraud, A. and Poeppel, D. (2013). &quot;Neuronal oscillations and speech perception: critical-band temporal envelopes are the essence.&quot; <em> Front. Hum. Neurosci.</em>  6:340. doi: 10.3389/fnhum.2012.00340<br />
[ <a href="downloads/peer-reviewed articles/GhitzaGiraudPoeppel_13.pdf" target="_new" class="linklg">PDF File</a> ]<br />
    <p><strong> Ghitza, O.</strong> (2012). &quot;On the role of theta-driven syllabic parsing in decoding speech: intelligibility of speech with a manipulated modulation spectrum.&quot; <em> Front. Psychol.</em> 3:238. doi:10.3389/fpsyg.2012.00238<br />
 [ <a href="downloads/peer-reviewed articles/Ghitza_12.pdf" target="_new" class="linklg">PDF File</a> ]<br /> 
    <p><strong> Ghitza, O.</strong> (2011). &quot;Linking speech perception and neurophysiology: speech decoding guided by cascaded oscillators locked to the input rhythm.&quot; <em>Front. Psychol.</em> 2:130. doi: 10.3389/fpsyg.2011.00130<br />
  [ <a href="downloads/peer-reviewed articles/Ghitza_11.pdf" target="_new" class="linklg">PDF File</a> ]</p>
    <p><strong>Ghitza, O</strong>. and Greenberg, S. (2009). &quot;On the possible role of brain rhythms in speech perception: Intelligibility of time compressed speech with periodic and aperiodic insertions of silence.&quot; <em>Phonetica</em> 66:113–126. doi:10.1159/000208934<br /> 
    [ <a href="downloads/peer-reviewed articles/1.pdf" target="_new" class="linklg">PDF File</a> ]</p>
      <p>Shamir, M.,<strong> Ghitza, O.</strong>, Epstein, S. and Kopell, N. (2009). &quot;Representation of time-varying stimuli by a network exhibiting oscillations on a faster time scale.&quot; <em>PLoS Comput Biol</em> 5(5). doi:10.1371/journal.pcbi.1000370<br />
    [ <a href="downloads/peer-reviewed articles/2.pdf" target="_new" class="linklg">PDF File</a> ] </p>
      <p>Messing, D. P., Delhorne, L., Bruckert, E., Braida, L. D. and <strong>Ghitza, O.</strong> (2009). &quot;A non-linear efferent-inspired model of the auditory system; matching human confusions in stationary noise.&quot; <em>Speech Communication</em> 51:668-683. doi:10.1016/j.specom.2009.02.002<br />
        [ <a href="downloads/peer-reviewed articles/3.pdf" target="_new" class="linklg">PDF File</a> ]  <br />
      </p>
      <p>Rix, A. W., Beerends, J. G., Kim, D.-S., Kroon, P. and <strong>Ghitza, O.</strong> (2006). &ldquo;Objective Assessment of Speech and Audio Quality – Technology and Applications,&rdquo; <em>IEEE Trans. Audio, Speech and Language Proc.</em>, SAP-14(6), 1890-1901<em><br />
      </em>[ <a href="downloads/peer-reviewed articles/4.pdf" target="_new" class="linklg">PDF File</a> ] </p>
      <p><strong>Ghitza, O. </strong>(2001). &ldquo;On the upper cutoff frequency of the auditory critical-band envelope detectors in the context of speech perception.&rdquo; <em>J. Acoust. Soc. Am.</em>, 110(3), 1628-1640<br />
      [ <a href="downloads/peer-reviewed articles/5.pdf" target="_new" class="linklg">PDF File</a> ] </p>
      <p><strong>Ghitza, O. </strong>and Sondhi, M. M. (1997). &ldquo;On the perceptual distance between speech segments.&rdquo; <em>J. Acoust. Soc. Am.</em>, 101(1), 522-529<br />
      [ <a href="downloads/peer-reviewed articles/6.pdf" target="_new" class="linklg">PDF File</a> ] </p>
      <p><strong>Ghitza, O. </strong>(1994). &ldquo;Auditory models and human performance in tasks related to speech coding and speech recognition.&rdquo; <em>IEEE Trans. on Speech and Audio</em>, SAP-2(1). Special issue on Neural networks for Speech Processing, 115-132 (Invited)<br />
      [ <a href="downloads/peer-reviewed articles/7.pdf" target="_new" class="linklg">PDF File</a> ] </p>
      <p><strong>Ghitza, O.</strong> (1993c). &ldquo;Processing of spoken CVCs in the auditory periphery: I. Psychophysics,&rdquo; <em>J. Acoust. Soc. Am.</em>, 94(5), 2507-2516<br />
      [ <a href="downloads/peer-reviewed articles/8.pdf" target="_new" class="linklg">PDF File</a> ] </p>
      <p> <strong>Ghitza, O.</strong> (1993b). &ldquo;Adequacy of auditory models to predict internal human representation of speech sounds.&rdquo; <em>J. Acoust. Soc. Am.</em>, 93(4), 2160-2171<br />
      [ <a href="downloads/peer-reviewed articles/9.pdf" target="_new" class="linklg">PDF File</a> ] </p>
      <p><strong>Ghitza, O. </strong>and Sondhi, M. M. (1993a). &ldquo;Hidden Markov Models with Templates as Nonstationary States: An Application to Speech Recognition.&rdquo; <em>Computer Speech and Language</em>, 7(2), 101-119<br />
      [ <a href="downloads/peer-reviewed articles/10.pdf" target="_new" class="linklg">PDF File</a> ]<br />
      </p>
      <p><strong>Ghitza, O.</strong> (1988). &ldquo;Temporal non-place information in the auditory nerve firing patterns as a front-end for speech recognition in a noisy environment.&rdquo; <em>Journal of Phonetics</em>, 16(1), 109-124.&nbsp; Theme issue on the &ldquo;Representation of speech in the auditory periphery&rdquo; (Invited)<br />
        <br />
<strong>Ghitza, O. </strong>(1987). &ldquo;Auditory nerve representation criteria for speech analysis/synthesis.&rdquo; <em>IEEE Trans. Acoust. Speech and Signal Proc</em>., ASSP-35(6), 736-740<br />
      [ <a href="downloads/peer-reviewed articles/12.pdf" target="_new" class="linklg">PDF File</a> ]      </p>
      <p><strong>Ghitza, O. </strong>(1986). &ldquo;Auditory nerve representation as a front-end for speech recognition in a noisy environment.&rdquo; <em>Computer Speech and Language</em>, 1(2), 109-131<br />
        <br />
<strong> Ghitza, O. </strong>and Goldstein, J. L. (1986).&nbsp; &ldquo;Scalar LPC quantization based on formant JNDs.&rdquo; <em>IEEE Trans. Acoust. Speech and Signal Proc</em>., ASSP-34(4), 697-709.<br />
        [ <a href="downloads/peer-reviewed articles/14.pdf" target="_new" class="linklg">PDF File</a> ]      <br />
  <tr>
    <td class="subhead"><strong>Most Significant Non Peer-Reviewed Publications</strong></td>
  </tr>
    <td class="bodylg"> 
    Lee, C-Y, Glass, J. and <strong>Ghitza, O.</strong> (2011). &ldquo;An efferent-inspired auditory model front-end for speech recognition.&rdquo; <em>Interspeech 2011</em>, 49-52, Florence, Italy, August<br />
     [ <a href="downloads/non peer-reviewed pubs/12.pdf" target="_new" class="linklg">PDF File</a> ] <br />
    <br/>
    Jepsen, M. L., Dau, T. and <strong>Ghitza, O. </strong>(2009). &ldquo;Modeling a damaged cochlea: beyond non-speech psychophysics.&rdquo; <em>International Symposium on Auditory and Audiological Research</em>, Copenhagen, Denmark, August<br />
      [ <a href="downloads/non peer-reviewed pubs/1.pdf" target="_new" class="linklg">PDF File</a> ] <br />
      <br />
      <strong>Ghitza, O.</strong> (2007). &ldquo;Using auditory feedback and rhythmicity for diphone discrimination of degraded speech.&rdquo; <em>Proceed. Intern. Conf. on Phonetics, ICPhS XVI</em>, 163-168, Saarbrücken, Germany, August<br />
      [ <a href="downloads/non peer-reviewed pubs/2.pdf" target="_new" class="linklg">PDF File</a> ] <br />
      <br />
      <strong>Ghitza, O.</strong> (2004). &ldquo;On the possible role of MOC efferents in speech reception in noise.&rdquo; <em>J. Acoust. Soc. Am.</em>, 115(5), A., 2500<br />
      [ <a href="downloads/non peer-reviewed pubs/3.pdf" target="_new" class="linklg">PDF File</a> ] <br />
      <br />
      <strong>Ghitza, O. </strong>and Kroon, P. (2000). &ldquo;Dichotic presentation of interleaving critical-band envelopes: An application to multi-descriptive coding.&rdquo; in <em>Proc. IEEE Speech Coding Workshop</em>, 72-74, Delavan, Wisconsin, September<br />
      [ <a href="downloads/non peer-reviewed pubs/4.pdf" target="_new" class="linklg">PDF File</a> ]<br />
      <br />
      <strong>Ghitza, O.</strong> and Sondhi, M. M. (1999).&nbsp; &ldquo;Perceptually motivated measures for automatic speech recognition.&rdquo; in <em>Proc. Robust Methods for Speech Recognition in Adverse Condition</em>, Tampere, Finland, May<br />
      [ <a href="downloads/non peer-reviewed pubs/5.pdf" target="_new" class="linklg">PDF File</a> ]      <br />
      <br />
      Kim, D. S., <strong>Ghitza, O. </strong>and Kroon, P. (1999). &ldquo;A computational model for MOS prediction.&rdquo; in <em>Proc. IEEE Speech Coding Workshop</em>, Provoo, Finland, June<br />
      [ <a href="downloads/non peer-reviewed pubs/6.pdf" target="_new" class="linklg">PDF File</a> ]      <br />
      <br />
      Sandhu, S., <strong>Ghitza, O. </strong>and Lee C-H. (1995). &ldquo;A comparative study of MEL Cepstra and EIH for phone classification under adverse conditions.&rdquo; <em>International Conference on Acoustics, Speech and Signal Processing – ICASSP '95</em>, 409-412, Detroit, May<br />
      [ <a href="downloads/non peer-reviewed pubs/7.pdf" target="_new" class="linklg">PDF File</a> ]      <br />
      <br />
      <strong>Ghitza, O. </strong>(1988). &ldquo;Auditory neural feedback as a basis for speech processing.&rdquo; <em>International Conference on Acoustics, Speech and Signal Processing – ICASSP '88</em>, 91-94, New York, April<br />
      [ <a href="downloads/non peer-reviewed pubs/8.pdf" target="_new" class="linklg">PDF File</a> ]      <br />
      <br />
      <strong>Ghitza, O. </strong>(1987). &quot;Robustness agaist noise: The role of timing-syncrony measurement.&rdquo; <em>International Conference on Acoustics, Speech and Signal Processing – ICASSP '87</em>, Dallas, April<br />
      [ <a href="downloads/non peer-reviewed pubs/9.pdf" target="_new" class="linklg">PDF File</a> ]      <br />
      <br />
      <strong>Ghitza, O.</strong> (1986). &ldquo;Speech analysis/synthesis based on matching the synthesized and the original auditory nerve representation.&rdquo; <em>International Conference on Acoustics, Speech and Signal Processing – ICASSP '86</em>, 2372-2375, Japan, April<br />
      [ <a href="downloads/non peer-reviewed pubs/10.pdf" target="_new" class="linklg">PDF File</a> ]      <br />
      <br />
      <strong>Ghitza,</strong><strong></strong> &quot;A measure of in-synchrony regions in the auditory nerve firing patterns as a basis for speech vocoding.&quot; <em>International Conference on Acoustics, Speech and Signal Processing – ICASSP '85</em>, 505-508, Tampa, March.<br />
      [ <a href="downloads/non peer-reviewed pubs/11.pdf" target="_new" class="linklg">PDF File</a> ] </p></td>
  </tr>
     <td class="subhead"><strong>Book Chapters</strong></td>
   <tr>
     <td class="bodylg"><strong>Ghitza, O. </strong>and Greenberg, S. (2010). &quot;Intelligibility of time-compressed speech with periodic and aperiodic insertions of silence: evidence for endogenous brain rhythms in speech perception?&quot; In: The Neurophysiological Bases of Auditory Perception (Eds.) E. A. Lopez-Poveda, A. R. Palmer, R. Meddis, Springer-Verlag, Berlin Heidelberg, 393-406<br />
       <strong><br />
       Ghitza, O.,</strong> Messing, D., Delhorne, L., Braida, L., Bruckert, E., and M. M. Sondhi (2007). &ldquo;Towards predicting consonant confusions of degraded speech.&rdquo; In: Hearing – from sensory processing to perception (Eds.) B. Kollmeier, G. Klump, V. Hohmann, U. Langemann, M. Mauermann, S. Uppenkamp and J. Verhey, Springer-Verlag, Berlin Heidelberg, 541-550<br />
       <br />
       <strong>Ghitza, O.</strong> (1994). &ldquo;Auditory models and human performance in tasks related to speech coding and speech recognition.&rdquo; In: Modern methods of speech processing (Eds.) R. P. Ramachandran, R. J. Mammone, Kluwer Academic Publishers, 401-448<br />
       <br />
       <strong>Ghitza, O. </strong>(1992).&nbsp; &ldquo;Auditory nerve representation as a basis for speech processing.&rdquo; In: Advances in speech signal processing (Eds.) S. Furui and M. M. Sondhi, Marcel Dekker, New York, 453-485<br />
       <br />
     <strong>Ghitza, O. </strong>and Goldstein, J. L. (1983). &ldquo;JNDs for the spectral envelope parameters in natural speech.&rdquo; In: Hearing – Physiological Bases and Psychophysics (Eds.) R. Klinke and R. Hartmann, Springer-Verlag, Berlin Heidelberg, 352-359.</td>
</table>


<table id="footer" width="700" height="100" border="0" cellpadding="5" cellspacing="5" align="center" class="footer">
<tr>
<td colspan="4"><img src="images/hr_rule.gif" height="1" width="712"></td>
</tr>
<tr>
<td width="371" valign="top" class="footer">&nbsp;</td>
<td width="103"><a href="http://www.bu.edu/bme/" target="_blank"><img src="images/footer_bu.gif" width="103" height="62" border="0" /></a></td>
<td width="74"><a href="http://cbd.bu.edu/" target="_blank"><img src="images/footer_cdb.gif" width="74" height="62" border="0" /></a></td>
<td width="90"><a href="http://www.bu.edu/hrc/" target="_blank"><img src="images/footer_bu_hearing_research.gif" width="122" height="62" border="0" /></a></td>
</tr>

</table>

</body>

</html>